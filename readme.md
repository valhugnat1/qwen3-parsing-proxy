# âœ¨ Qwen3 Parsing Proxy

A FastAPI-powered proxy server for OpenAI-compatible APIs (like OpenAI or Fireworks AI) that parses special XML-like tags in model responsesâ€”`<tool_call>` and `<think>`â€”to enable structured function calling and introspection.

---

## ğŸš€ Features

- **OpenAI-Compatible Endpoints**  
  Drop-in replacement for `/v1/chat/completions` and `/chat/completions`.

- **ğŸ”§ Tool Call Tag Parsing**  
  - Detects and parses `<tool_call>{ "name": "...", "arguments": { ... } }</tool_call>`.
  - Removes the tag from the `content`.
  - Adds a `tool_calls` array to match OpenAI's spec.
  - Automatically sets `finish_reason: "tool_calls"`.
  - Gracefully handles malformed or incomplete tags.

- **ğŸ’­ Think Tag Parsing**  
  - Extracts content from `<think>...</think>`.
  - Removes it from `content`.
  - Appends extracted data to a non-standard `reasoning_content` field.

- **ğŸ“¡ Streaming Support**  
  - Fully supports streaming with `stream: true`.
  - Emits SSE (`text/event-stream`) with incremental deltas for `content` and `tool_calls`.

- **ğŸ§  Native Tool Call Passthrough**  
  - Supports tool calls directly generated by the upstream provider.

- **ğŸ§¼ Clean Architecture**  
  - Modular codebase: routing, core services, models, config.

- **âš™ï¸ Easy Configuration**  
  - `.env`-based setup for host, port, and API keys.

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8+
- `pip`

### Setup

```bash
git clone <your-repo-url>
cd <repository-directory>
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate
pip install -r requirements.txt
```

---

## âš™ï¸ Configuration

Create a `.env` file in the root directory:

```bash
cp .env.example .env
```

### `.env` Variables

```dotenv
# Required
FIREWORKS_API_KEY=your_fireworks_api_key_here

# Optional
# OPENAI_API_KEY=your_openai_api_key_here
# HOST=0.0.0.0
# PORT=8000
```

---

## ğŸ Running the Server

### Basic Run

```bash
python main.py
```

### Development Mode (Hot Reload)

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

---

## ğŸ“¬ Usage

### Standard Request (Non-Streaming)

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $YOUR_API_KEY" \
  -d '{
    "model": "accounts/fireworks/models/firefunction-v1",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is the weather like in Metz, France?"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "get_current_weather",
          "description": "Get the current weather in a given location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": { "type": "string" },
              "unit": { "type": "string", "enum": ["celsius", "fahrenheit"] }
            },
            "required": ["location"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

### Example Response

If the model returns:

```
Thinking... <think>User wants the weather in Metz, France.</think>
Okay. <tool_call>{ "name": "get_current_weather", "arguments": { "location": "Metz, France", "unit": "celsius" } }</tool_call>
```

The proxy responds with:

```json
{
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "Okay.",
        "tool_calls": [
          {
            "id": "call_...",
            "type": "function",
            "function": {
              "name": "get_current_weather",
              "arguments": "{\"location\": \"Metz, France\", \"unit\": \"celsius\"}"
            }
          }
        ],
        "reasoning_content": "User wants the weather in Metz, France."
      },
      "finish_reason": "tool_calls"
    }
  ]
}
```

---

### Streaming Request

```bash
curl -N http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $YOUR_API_KEY" \
  -d '{
    "model": "accounts/fireworks/models/firefunction-v1",
    "messages": [{"role": "user", "content": "Call the dummy function."}],
    "stream": true
  }'
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .env                  # Environment variables
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ main.py               # App entry point
â””â”€â”€ app/
    â”œâ”€â”€ api/              # API routes (FastAPI routers)
    â”œâ”€â”€ core/             # Config, client initialization
    â”œâ”€â”€ models/           # Pydantic schemas
    â””â”€â”€ services/         # Parsing logic, streaming handlers
```

---

## ğŸ§° Tech Stack

- **FastAPI** â€” Web API framework  
- **Uvicorn** â€” ASGI server  
- **Pydantic** â€” Data models & validation  
- **OpenAI SDK** â€” Communication with downstream LLMs  
- **python-dotenv** â€” Environment config loader  
